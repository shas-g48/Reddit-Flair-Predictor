1. set up full training + eval skeleton
	separated training and validation data N/5
2. Pick a simple model that can't be screwed up
	Picked up a lstm
		Simplifications:
			embedding size set to 50
			Input size set to 25
			GRU hidden units set to 30
3. Fixed random seed to 10
4. No fanciness in data
5. Set up eval with metric
	I can measure performance by (tensorboard):
		loss over batches of training
		total acc over complete validation set at every epoch
		acc by cat over complete validation set at every epoch
6. Verify starting loss
	Should be 2.48 getting 2.87 (need to find out why) (but this is mean)
	Verified loss function by manually finding loss.
	Verified output logits dims
7. Set correct bias for logits
	found the bias but adding it would mean not using library's
	softmax cross entropy to manually add bias in probabilities,
	or figuring out the correct bias to add in the logits, which
	can be done but network can learn bias on it's own
8. Verify input independent baseline
	input independent baseline (max from graph) (25 epochs)
		total: 4285
		cat0: 724
		cat1: 4214
		cat2: 963
		cat3: 161
		cat4: 53
		cat5: 10
		cat6: 1
		cat7: 1
		cat8: 0
		cat9: 2
		cat10: 0
		cat11: 6
	When actually feeding in data:
		total: 5875
		cat0: 3016
		cat1: 3766
		cat2: 460
		cat3: 120
		cat4: 8
		cat5: 1
		cat6: 1
		cat7: 1
		cat8: 0
		cat9: 0
		cat10: 0
		cat11: 3
	Inference:
	How I know the network is learning something from the data:
		1. in input independent case, it is unable to increase
		total accuracy beyond a level, first it just learns the
		dataset output bias (the raw probabilities of occurence
		of each category), then it goes clueless as it does not
		know what signal is coming from where.
		2. in input independent case changes in category
		accuracy are less drastic than when input is actually
		fed, because the network actually has same input 
		for all sentences, whereas when input is actually fed,
		it greedily increases accuracy of a few categories to get a
		higher total accuracy as it is able to identify the
		input. This can also be seen by the much higher value
		of cat0 in actual input than input independent, it
		can be more aggressive as it knows where the signal is coming
		from
	Some roadblocks:
		The network is doing a good work on total accuracy, 
		but at the cost of decreasing accuracy classes other
		than majority. As this is not the actual architecture
		I'll use, I'm not taking a action now and will see if
		the network behaves well in the future, if it does
		not I'll have to tackle the class imbalance directly.
	More analysis done:
		I reduced seq length from 25 to 10, as I had found
		during eda that on avg seq length is diff for the classes
		I got similar results to actual data with 25 words, 
		but less max values, which tells that the network is
		not just working on the length of the data
	Possible errors in current analysis:
		Maybe the smaller data does not represent the actual
		data distribution as I just did:
			head -n <nu_of_lines> data > smaller_data
		Will be checking if the distribution is indeed correct.
		My doubt was true, the counts were:
			[97, 97, 0, 0, 8, 25, 0, 9, 0, 0, 1, 13]
		That is why the cat2 went to zero!
		Now I'll manually create smaller datasets from the training 
		by following similar method as done in eda
	Action taken:
		Separate dataset and do 6-8 again
6-2. Verify starting loss
	Should be 2.48 and got 2.48
7-2. Set correct bias for logits
	Skipping again, same argument as 7.
8-2. Verify input independent baseline
	Note: dataset used for training: data_correct_med.txt
			Stats: [3916, 3572, 2304, 774, 1310, 724, 92, 327, 181, 340, 6, 346]
			Total: 13892
	      dataset used for validation: data_valid.txt
			Stats: [4895, 4465, 2881, 967, 1638, 904, 114, 409, 226, 426, 8, 433]
			Total: 17366
	input independent baseline (25 epochs) (max, last)
		total: 3224, 3101 (decreasing)
		cat0: 701,701 (strictly increasing)
		cat1: 2305,1986 (strictly decreasing)
		cat2: 530, 370 (strictly decreasing)
		cat3: 27, 16 (decreasing)
		cat4: 36, 28 (increasing, bell shape)
		cat5: 0, 0 (constant)
		cat6: 1, 0 (decreasing)
		cat7: 0, 0 (constant)
		cat8: 0, 0 (constant)
		cat9: 0, 0 (constant)
		cat10: 0, 0 (constant)
		cat11: 0, 0 (constant)
		acc % (calculated manually) = 18.5%
	When actually feeding in data:
		total: 9405, 9405 (strictly increasing)
		cat0: 3714, 3595 (decreasing, fluctuating at a high value)
		cat1: 2434, 2399 (increasing)
		cat2: 2064, 2054 (increasing)
		cat3: 199, 199 (strictly increasing)
		cat4: 817, 787 (increasing, bell shape)
		cat5: 87, 87 (strictly increasing)
		cat6: 63, 63 (increasing)
		cat7: 118, 118 (strictly increasing)
		cat8: 0, 0 (constant)
		cat9: 93, 93 (strictly increasing)
		cat10: 0, 0 (constant)
		cat11: 10, 10 (increasing)
		acc % (calculated manually) = 54.15%
	Inference:
	How I know the network is learning something from the data:
		1. Total accuracy increases when actual data is fed and 
		decreases when zeros are fed instead. Maximium and final
		both are much higher than input independent baseline.
		2. When feeding in zeros, model just learns to output
		dominant class, increasing its accuracy, while almost all
		others decrease. This is the most efficient ways when you
		don't know anything about the input, just output class with
		max in batch, while when input is actually fed, network
		can see where the signal is coming from, and this
		results in increase in accuracy among almost all cats
	Possible roadblocks:
		1. cat10 and cat8 remain constant at zero.
		2. There is very little data for cat10
		3. There is less data for cat8, but this is also 
		true for cat6, so the problems can be:
			1. Title length of cat8 is too small, or mostly noise
			2. There is very little structure in cat8 titles
			3. some bug in the code
	Action Taken:
		Not taking action now, but if same behavior, will conduct eda again
		(unable to fit cat8 even when less imbalance)
		For cat10 data size is mostly at fault in my opinion
9. Overfit one batch
	Method: Set training set and validation set same, to a small dataset
		Wait till loss -> zero
		See that accuracy is maximum
	Note: Same dataset used for training and validation (data_small_tiny.txt)
		Stats: [56, 52, 33, 12, 19, 11, 2, 5, 3, 5, 1, 5]
		Total: 204
		lr = 1e-3
	Result:	The model completely fit the training data and loss went down to 0.01
	Inference: The model is capable of reaching zero loss on a tiny dataset, no
		major bug is there in the code
10. Verify decreasing training loss:
	Method: Verify underfitting in current model on a medium size dataset
		This happens when model is unable to cross a certain accuracy level
		due to lake of available weights. Can be seen easily by decreasing lr
		so that wiggling does not happen due to a large lr, and seeing if
		loss and accuracy starts fluctuating and reaches some level which
		it cannot break through in a finite time period.
	Note: Same dataset for training and validation (data_small_keep_all_cats.txt)
		Stats: [1958, 1786, 1152, 387, 655, 362, 46, 164, 91, 170, 3, 173]
		Total: 6947
		lr: 3e-4
		gru hidden units = 30
		epochs = 500
	Result: Model struggles to classify the last 1000 points
		Accuracy starts fluctuating at about 240 epochs (stops increasing strictly)
		Loss is unable to go below 0.3 and starts fluctuating around 0.45
		Maximum accuracy 6020 at 500 epochs
	Claim: Increasing model capacity just a little will reach same accuracy and loss in less time.
	Note: Kept all other params same but increased gru hidden unit size to 50
	Result: Model reaches maximum accuracy 6913 at 483 epochs
		Loss goes much lower than 0.3 reaching less than 0.05 and even lower avg loss sometimes
		Model exceeds previous baseline by reaching 6103 in just 133 epochs.
	Additional Notes: While increasing from 30 -> 50 is increasing the number of weights too much,
		increasing it a little bit will also give gains albeit less
	Inference: Model is underfitting in current capacity (30 hidden units) on 6947 data points
		Increasing the model size leads to a decrease in loss and increase in accuracy.
11. Visualize all inputs to model just before it enters it
	Verified encoding of output to one hot in data loader
	Verified dims of input: (16, 25, 50) (batch_size, time_steps, embedding_size)
	Verified dims of output: (16, 12) (batch_size, no_of_cats)
	All inputs can be visualized just before entering by doing dimensionality reduction and 
	plotting on a graph. I know distribution of output from eda, and distribution of input
	depends on the size of embedding used and the specific words. By plotting this and using
	the labels, maybe I can find which words are more important for which category.
	Action taken:
		Skipping for now as this can be done by our intuition about language, but this can be 
		useful to find things we don't expect to change the output.
12. Visualize Prediction dynamics:
	Visualizing previous graphs, The loss graph of gru with 50 units indicates a optimal loss
	function decrease, but can be sped up by increasing the lr
	The loss graph of gru with 30 units indicates a low lr initially and underfitting later
13. Check for mixing across batch/ use backprop to chart dependencies
	I checked the dims of all 'checkpoints' in the graph and they seem fine
	Then I went to tensorboard graphs to check if the connections to gradients
	were as expected, and they were. Only output is Adam and inputs are rnn, dense,
	loss and inference as expected.
14. Generalize a special case, write loopy code before vectorized
	Tensorflow performs better with vectorized code, and already I have checked most 
	of the 'checkpoint' by setting them as self.'node to check' = 'node to check' and 
	then printing the result after retrieving it from sess.run()
extra. Overnight baseline on big dataset:
	Note: Dataset for training: data_train.txt 
		Stats: [19578, 17855, 11519, 3865, 6548, 3615, 455, 1632, 902, 1699, 27, 1727]
		Total: 69422
	      Dataset for validation: data_valid.txt
		Stats: [4895, 4465, 2881, 967, 1638, 904, 114, 409, 226, 426, 8, 433]
		Total: 17366
		lr = 6e-4
		epochs = 537
		hidden units = 70
	Result: Network achieves maximum accuracy of 10145 of 17366 (0.578) at 22 epochs
		and then falls down to ~9600 of 17366 ((0.552) after 50 epochs and never seems
		to escape from 9300-10000 accuracy, fluctuating in the range. 
		The loss achieves a minimum of ~0.3 for a single epoch but after smoothing the curve,
		it never goes below 0.8, which shows signs of underfitting. (lr was set low because
		I didn't want it to fluctuate because of a high lr, and the epochs were set high so that
		the network had sufficient time to change despite the low lr)
		All this tells that the current network architecture does not have the capacity
		to even learn the training set completely.
		At 537 ep, correct by cat: [[3536 2363 1786  242  881  204   70  167   25  126    0  158]]

15. Choose a correct architecture for the data by focusing on the training loss
	Method: Find the decrease in training loss for same number of epochs
		for network architectures that take the same time to train (counting 
		no of weights is better but time matters much more in my opinion
		to compare similar structures)
		Right now I'm trying to find the 'structure' that works best to decrease
		the loss.
		Time to train one epoch: 11 sec (on cpu)
		Total time: 11 + 3 = 14 sec
		Time to test 50 epochs = 12 mins
		lr = 1e-3
	Models to iterate on:
		gru single layer
		gru multiple layer
		gru multiple layer with different no of hidden units
		increase final fc capacity only
		bidirectional rnn
		all the above with attention
		I'll measure decrease in loss and no of epochs to achieve it
		while right now gru multiple + attention seems tempting, only the analysis
		can tell what model to choose
		Also I can test the embedding size's effect on performance, there is already
		a paper on it I found.
		I am using the same dataset for training and testing, as I have to see which overfits better
	Tested:
		Test1: gru 30 hidden units simple
			lowest loss = 1.022 (start loss 1.624) (delta = 0.602)
			epochs to reach this = 50 (used all)
			total acc max = 8751 (46 epochs)
		Test2:	gru 15 hidden units, 2 layer stacked (time to train one ep ~12-13 sec)
			lowest loss = 1.138 (start loss 1.674) (delta = 0.536)
			epochs to reach this = 50 (used all)
			total acc max = 8528 (50 epochs)
			Inference: I thought this would perform better as it would learn a higher
			representation of the data, maybe the issue is data bottle neck from the bottom
			layer, lets increase bias towards input
		Test3: gru bias towards input 20+10 hidden units in layers (time to train one ep ~12-13 sec)
			lowest loss = 1.113 (start loss 1.685) (delta = 0.572)
			epochs to reach this = 50 (used all)
			total acc max = 8679 (50 epochs)
			Inference: Better than 2 but worse than 1)
		Test4: gru bias towards output (should be worse than all) 10+20
			lowest loss = 1.169 (start loss 1.655) (delta = 0.486)
			epochs to reach this = 50 (used all)
			total acc max = 8313 (50 epochs)
			Inference: As expected, worst performance till now
		Test5: gru encoder decoder [12,7,11]
			lowest loss = 1.241 (start loss 1.769) (delta = 0.528)
			epochs to reach this = 50 (used all)
			total acc max = 8138 (50 epochs)
			Inference: Even worse due to information bottleneck in getting input
		Test6: increase fc + use relu in middle layer (30 hidden, 60 dense + relu , 12 output dense)
			lowest loss = 0.915 (start loss 1.587) (delta = 0.672)
			epochs to reach this = 50 (used all)
			total acc max = 9149 (47 epochs)
			Inference: The best model till now, combination of 1 + additional fc
			This seems to give a nice performance boost without much performance penalty
			(time to train a epoch ~10.5 sec near to 1.)
			Will test different activations and also will increase hidden size
			to check if there is still boost with more hidden units.
			If it is true will include layers in final fc.
		Test7: increase fc and no activation
			lowest loss = 1.005 (start = 1.585) (delta = 0.58)
			epochs to reach this = 50 (used all)
			total acc max = 8972 (48 epochs)
			Inference: Worser than fc + relu, will check tanh
		Test8: increase fc + tanh in middle layer
			lowest loss = 0.945 (start loss = 1.575) (delta = 0.63)
			epochs to reach this = 50 (used all)
			total acc max = 9278 (50 epochs)
			Inference: It was able to get a result on cat10 surprised me, 
			which relu didn't catch despite lower final loss. 
			Again, Performance gain is good with very little penalty
		Test9: bidirectional rnn (15 hidden unit each in fw, bw)
			lowest loss = 1.172 (start loss = 1.687) (delta = 0.515)
			epochs to reach this = 50 (used all)
			total acc max = 8284 (50 epochs)
			Inference: Decreasing hidden size just to have bidirectional 
			does not seem logical, maybe this will change when hidden size increases
			and gain from having more < having 2 directions, but for now, it has the 
			worst performance
		Test10: attention + simple gru (time to train one epoch = 11-12 sec)
			lowest loss = 0.981 (start loss = 1.596) (delta = 0.615)
			epochs to reach this = 50 (used all)
			total acc max = 9073
			Inference: Better than simple gru, But I am skeptical as I have implemented 
			this attention mechanism from scratch, Maybe this performance increase
			is just due to increase in final output size, the increase in total acc
			is a good sign that attention is working, but loss should have been much lower.
			Compared to simple gru, cat8 received a boost, graphs of cat0, and cat2 show
			completely opposite behavior, cat9 seems completely unaffected, cat10 is still
			zero.
			I'll combine the sota till now with this just for fun to see what happens.
		Test11: attention + simple gru + 60 dense + relu (time to train one epoch = 12-13 sec)
			lowest loss = 0.8705 (start loss = 1.602) (delta = 0.7315)
			epochs to reach this = 50 (used all)
			total acc max = 9225 (50 epochs)
			Inference: Better performance than just atttention and just relu, the lowest
			loss till now. Both were not able to catch cat11 and neither was this able
			to catch it. Combining attention with tanh should give higher loss 
			than this but better accuracy. I am still skeptical about performance of
			my attention module, and Test 13 will check it
		Test12: attention + simple gru + 60 dense + tanh (time to train one epoch = 12-13 sec)
			lowest loss = 0.856 (start loss 1.602) (delta = 0.746)
			epochs to reach lowest loss = 50 (used all)
			total acc max = 9328 (48 epochs)
			Inference: Loss also is the lowest, which I had not expected. Total acc is 
			maximum, which I had predicted already. But this does not detect cat11,
			while pure tanh does.
		Test13: Attention skepticism test:
			First get input independent baseline
				Note: Input is being fed, but both attention and gru outputs are masked to
					zero
				lowest loss = 1.906 (start loss = 2.242)
				epochs to reach this = 50 epochs
				total acc max = 3916 (0 epochs)
				Inference: Network has constant total acc, which is just the total no of data points
				having majority in training (same as testing as both datasets are same). It just
				learns to output majority all the time.
			Test 13.1: Mask attention output to zero, acc should be near simple gru
				lowest loss = 1.004 (start loss = 1.639)
				epochs to reach this = 50 epochs
				total acc max = 9024 (50 epochs)
				Inference: Loss is near simple gru, while total acc increased. This maybe due
				to increase of weights alone. Acc is close to attention.
			Test 13.2: Mask gru output to zero, acc should be better than input independent
				lowest loss = 1.035 (start loss = 1.649)
				epochs to reach this = 50 epochs
				total acc max = 8685 (50 epochs)
				Inference: Attention has some data which works even when 
				last output of rnn is masked out.	
			Test 13.3: Random numbers instead of attention output should be worse than actual, maybe even					
				lower than simple gru
				Note: The random numbers are same across rows, as broadcasting is used to get
				random vector of current batch size
				lowest loss = 1.004 (start loss = 1.631)
				epochs to reach this = 50 epochs
				total acc max = 8828 (43 epochs)
				Inference: Both metrics are worse than simple gru with attention as expected
			Additional Note: The attention module is not actual attention, as we cannot access the hidden
				state of every cell using tf.dynamic_rnn api, I am using the outputs at each timestep
				instead. Actual attention will work much better. As this is just a hacked together
				solution, I'll use it only if the results compel me to, and similar performance gain
				can't be achieved by other means.
		Test14: multiple fc layers. I can count weights easily so this comparison
			will be fair.
			Test 14.1 add 60 + 60 dense (weights = (30*60) + 60 + (60*60) + 60 + (60*12) + 12 = 6252
				lowest loss = 0.9618 (initial = 1.563) (delta = 0.6012) 
				epochs to reach this = 50
				total acc max = 8948 (50 epochs)
			Test 14.2 add x dense (weights = (30*x) + x + (x*12) + 12 = 6252 x=145.11, 145 gives 6247,
			much closer than 6290
				i.e. add 145 dense
				lowest loss = 0.948 (initial = 1.577) (delta = 0.629)
				epochs to reach this = 50
				total acc max = 9181 (50 epochs)
			Inference: The first layer form gru output can use a expansion in size, by using a big
			dense layer. 

			Additional Note: I forgot to set random seed for python program wide and only set tensorflow 
			and numpy's random seed. Hence I cannot reproduce the above results again. I will try to 
			update tests that were too close with fixed seed. Changing the seed has an effect on performance,
			but if system is robust, this effect won't depend too much on the seed. Still a fixed seed
			is much better if someone wants to replicate the results exactly each time.
			What this means: Many of the fringe cases above may be just due to having a better
			random seed by chance. Like tanh proving better than relu for finding cat11. These
			cases need re-evaluation.
			What this also means that if I re-evaluate all results for a single seed, they may
			not be same as if I did them with a different seed.
			I can remove the randomness, but then again order in which data is fed can affect results.
			The idea is not to take the above results as too rigid.

