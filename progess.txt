8 April 2020:
first tried to use praw, but found that it can only get 1000 results, which amounts to ~3 days of data on r/india
Using another api pushshift, able to search posts by date, but it had the following problem:
	pushshift unlike praw is not using reddit api directly, pushshift instead captures a post
	when it is initially posted. This means that if the post changes, these changes are not captured by the api
	for example if it is removed by moderators, automod removes it, it gets deleted, has missing/ incorrect flair etc.
So I went for a hybrid approach:
	1. Use pushshift to get results by searching for posts after a date, iterating over date(s)
	2. Use praw to get current status of the post by getting it's url from pushshift
	3. if valid, get data from praw

10 April 2020:
Conducted initial eda, came to know a lot about the dataset, cleaned it
Class imbalance came to light, and so did the imbalance in data length available 
for each class
Right now I am thinking of trying out some of the following approaches:
1. Use lstm with/without attention with text embeddings from fasttext
2. Finetune a recurrent net, like ULMFiT
3. Finetune a transformer net, like openAI GPT
4. Ensemble? Boost data? Under/Over sample?
Currently proceeding to understand how to do the above, all this + eda 
will be done again and again till get a satisfying answer

12 April 2020
Trying to get baseline accuracy using transformers, found a simple implementation SimpleTransformers https://github.com/ThilinaRajapakse/simpletransformers
plan to do a baseline lstm, maybe from scratch? and also metric for loss to tackle imbalance.

13 April 2020
Trying gru from scratch + embeddings from fasttext

14 April 2020 noon
gru from scratch implemented
some roadblocks:
1. instead of loading whole dataset and converting to embedding, need to only get embedding for batch
2. need to use embeddings for subword rather than word as won't generalize to oov
3. came across gensim? can maybe use that
4. better loss function than softmax cross entropy

14 April 2020 night
gru from scratch + fasttext + input/output placeholders implemented
some roadblocks faced:
1. Preferred approach was to use tf.data, but cannot because converting the whole dataset to embedding runs out of memory.
2. Then tried tensorflow eager, able to get data but for some reason unable to apply gradients
3. Then implemented non eager tf with placeholder (using gensim to train fasttext model), training on a dataset of ~80k is too slow to be practical
4. Can use a embedding layer, but then won't generalize to oov
Todo:
1. Figure out a way to make training feasible before implementing ideas from 14 April
