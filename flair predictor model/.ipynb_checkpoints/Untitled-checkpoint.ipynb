{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "\n",
    "def load_vectors(filename):\n",
    "    \"\"\"\n",
    "    Creates vectors for vocabulary\n",
    "    in a dict of form {word: embedding}\n",
    "\n",
    "    filename: file containing vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # load embeddings according to fasttext format\n",
    "    filein = io.open(filename, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    embeds = {}\n",
    "    for line in filein:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        embeds[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "    # add the embedding for space\n",
    "    embeds[' '] = [0.016601462,0.0067870123,-0.028963935,0.016884321,-0.008362424,-0.025133897,-0.027598584,-0.01069816,-0.00077911816,-0.023480771,-0.015628096,-0.027175352,0.027422002,0.013819963,0.02896208,-0.02149246,0.015003212,0.006882444,0.01330033,0.0013655132,0.029585376,0.0045530014,0.0024796196,-0.011898544,0.011689002,-0.014970597,-0.00570085,-0.01727686,0.030820029,-0.009950199,-0.0030393065,0.0006726758]\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(input_batch, en):\n",
    "    \"\"\"\n",
    "    Convert input batch to embeddings\n",
    "\n",
    "    input batch: batch consisting of sentences\n",
    "    en: word vector dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    # hold batch converted to embeddings\n",
    "    embed_output = []\n",
    "\n",
    "    # convert one line at a time\n",
    "    for line in input_batch:\n",
    "        cur_line = []\n",
    "        # add embeddings for each word in sentence\n",
    "        for word in line:\n",
    "            cur_line.append(en[str(word)])\n",
    "        # add sentence to batch\n",
    "        embed_output.append(cur_line)\n",
    "\n",
    "    return np.asarray(embed_output, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_mapping():\n",
    "    \"\"\" Produces mapping from flair to index and vice versa \"\"\"\n",
    "    valid_flairs = ['Politics', 'Non-Political', 'AskIndia', 'Policy/Economy',\n",
    "                    'Business/Finance','Science/Technology', 'Scheduled',\n",
    "                     'Sports', 'Food','Photography','CAA-NRC-NPR', 'Coronavirus']\n",
    "\n",
    "    # create the mappings in separate dictionaries\n",
    "    flair_to_index = dict()\n",
    "    index_to_flair = dict()\n",
    "    for index, flair in enumerate(valid_flairs):\n",
    "        flair_to_index[flair] = index\n",
    "        index_to_flair[index] = flair\n",
    "\n",
    "    return flair_to_index, index_to_flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, time_steps):\n",
    "    \"\"\"\n",
    "    Load dataset from file\n",
    "\n",
    "    filename: file containing dataset\n",
    "    time_steps: no of words to get for each data point\n",
    "    \"\"\"\n",
    "    # get mapping from flair to index\n",
    "    flair_to_index, _ = flair_mapping()\n",
    "\n",
    "    # input and output arrays for data\n",
    "    load_input = []\n",
    "    load_output = []\n",
    "\n",
    "    # a variable to select which array to write to\n",
    "    select_list = 0\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # add flair index to output and toggle\n",
    "            if select_list == 0:\n",
    "                load_output.append(flair_to_index[line[:-1]])\n",
    "                select_list = 1 # the next line is input\n",
    "\n",
    "            # add sentence to input and toggle\n",
    "            elif select_list == 1:\n",
    "                # hold current input\n",
    "                cur_input = []\n",
    "\n",
    "                # get number of words in current line\n",
    "                len_in = len(line[:-1].split()[:time_steps])\n",
    "\n",
    "                # add spaces before the sentence to get total length to timesteps\n",
    "                for _ in range(time_steps - len_in):\n",
    "                    cur_input.append(' ')\n",
    "\n",
    "                # add the words to complete the input line\n",
    "                for word in line[:-1].split()[:time_steps]:\n",
    "                    cur_input.append(word)\n",
    "\n",
    "                load_input.append(cur_input)\n",
    "\n",
    "                select_list = 0 # the next line is output\n",
    "                \n",
    "    # hold outputs in one hot format\n",
    "    one_hot_output = []\n",
    "    # convert output flair indices to one hot, one output at a time\n",
    "    for output_flair_index in load_output:\n",
    "        # generate zeros of size no of flairs, and one hot it according to index\n",
    "        one_hot = [0 for i in range(12)]\n",
    "        one_hot[output_flair_index] = 1\n",
    "\n",
    "        # append to output array\n",
    "        one_hot_output.append(one_hot)\n",
    "\n",
    "    return (np.asarray(load_input, dtype=np.object), np.asarray(one_hot_output, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_mkdir(path):\n",
    "    \"\"\"\n",
    "    Creates directories if they don't exist\n",
    "\n",
    "    path: relative path to directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import ceil\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated (<ipython-input-56-1d4a521fcb4f>, line 144)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-1d4a521fcb4f>\"\u001b[0;36m, line \u001b[0;32m144\u001b[0m\n\u001b[0;31m    dtype=tf.int32)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated\n"
     ]
    }
   ],
   "source": [
    "class FlairPredict(object):\n",
    "    def __init__(self):\n",
    "        # learning rate\n",
    "        self.lr = 1e-3\n",
    "\n",
    "        # get vectors for vocabulary used while training\n",
    "        self.en = load_vectors('data/vocab.vec')\n",
    "\n",
    "        # batch size for training and validation\n",
    "        self.batch_size_train = 32\n",
    "        self.batch_size_valid = 10000\n",
    "\n",
    "        # to print loss at regular intervals\n",
    "        self.skip_step = 50\n",
    "\n",
    "        # timesteps to take in data for\n",
    "        self.time_steps = 50\n",
    "\n",
    "        # embedding size used\n",
    "        self.embed_size = 32\n",
    "\n",
    "        # count number of times above\n",
    "        # desired validation accuracy\n",
    "        self.above_acc = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        # hold maximum validation accuracy\n",
    "        # and no of times it was lower than\n",
    "        # the latest max validation accuracy\n",
    "        self.max_acc = 0\n",
    "        self.overfit = 0\n",
    "        \n",
    "        with tf.variable_scope('jupyter_specific', reuse=tf.AUTO_REUSE) as scope:\n",
    "            # step count for summaries used in tensorboard\n",
    "            self.gstep = tf.get_variable('gstep', dtype=tf.int32,\n",
    "                                    initializer=tf.constant(0))\n",
    "            self.gstep_acc = tf.get_variable('gstep_acc', dtype=tf.int32,\n",
    "                                    initializer=tf.constant(0))\n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\" Load the dataset, and make placeholders for input and output \"\"\"\n",
    "        with tf.variable_scope('data', reuse=tf.AUTO_REUSE) as scope:\n",
    "            # Load datasets for training and validation\n",
    "            self.train_input, self.train_output = load_data('data/data7_train_more.txt',\n",
    "                                                                    self.time_steps)\n",
    "            self.valid_input, self.valid_output = load_data('data/data7_valid.txt',\n",
    "                                                                    self.time_steps)\n",
    "\n",
    "            # get validation set size, used for printing progress\n",
    "            self.size_valid = len(self.valid_input)\n",
    "\n",
    "            # placeholders for input and output\n",
    "            self.input = tf.placeholder(dtype=tf.float32, shape=[None, self.time_steps, self.embed_size])\n",
    "            self.output = tf.placeholder(dtype=tf.int32, shape=[None, 12])\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\" Define the inference part of the graph \"\"\"\n",
    "        with tf.variable_scope('inference', reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            # placeholders for dropout rates\n",
    "            self.last_output_rate = tf.placeholder(dtype=tf.float32, shape=())\n",
    "            self.dense1_rate = tf.placeholder(dtype=tf.float32, shape=())\n",
    "            self.dense2_rate = tf.placeholder(dtype=tf.float32, shape=())\n",
    "\n",
    "            # gru cell\n",
    "            cell = tf.keras.layers.GRUCell(\n",
    "                        units=50, kernel_regularizer=tf.keras.regularizers.l2(0.6),\n",
    "                        recurrent_regularizer=tf.keras.regularizers.l2(0.6)\n",
    "                        )\n",
    "\n",
    "            # attn = tf.contrib.rnn.AttentionCellWrapper(cell, attn_length=7)\n",
    "\n",
    "            # unroll the gru cell and get rnn outputs\n",
    "            rnn = tf.keras.layers.RNN(cell, return_sequences=True, return_state=True)\n",
    "            seq_output, out_state = rnn(self.input)\n",
    "\n",
    "            # get output from the end of rnn\n",
    "            last_output = seq_output[:, self.time_steps - 1, :]   # (bs, hidden_size)\n",
    "\n",
    "            # dropout for output from end of rnn\n",
    "            last_output_drop = tf.nn.dropout(last_output, rate=self.last_output_rate)\n",
    "\n",
    "            # dense layer 1 with dropout\n",
    "            dense1 = tf.layers.dense(last_output_drop, 120, activation=tf.nn.tanh,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.6))\n",
    "            dense1_drop = tf.nn.dropout(dense1, rate=self.dense1_rate)\n",
    "\n",
    "            # dense layer 2 with dropout\n",
    "            dense2 = tf.layers.dense(dense1_drop, 240, activation=tf.nn.tanh,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.6))\n",
    "            dense2_drop = tf.nn.dropout(dense2, rate=self.dense2_rate)\n",
    "\n",
    "            # output logits\n",
    "            self.logits = tf.layers.dense(dense2_drop, 12, activation=None,\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.6))\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Defines loss used to train\n",
    "        softmax cross entropy with logits is used to compute loss\n",
    "        between network output and the correct one hot output\n",
    "        \"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            # get the loss for each input\n",
    "            log_prob = tf.nn.softmax_cross_entropy_with_logits(labels=self.output,\n",
    "                                                                logits=self.logits) # (bs, 12)\n",
    "\n",
    "            # take mean across inputs\n",
    "            self.loss = tf.reduce_mean(log_prob)\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\" Defines optimizer operation \"\"\"\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE) as scope:\n",
    "            self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss,\n",
    "                                                            global_step=self.gstep)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Defines operations to find accuracy\n",
    "\n",
    "        self.acc: No of correct results in whole validation set\n",
    "        self.cat_acc: No of correct results in whole validation set by flair\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('evaluate', reuse=tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            # hold total accuracy and accuracy by category (flair)\n",
    "            self.acc = tf.get_variable('acc', initializer=tf.constant(0), dtype=tf.int32)\n",
    "            self.cat_acc = tf.get_variable('cat_acc', shape=(1,12),\n",
    "                                            dtype=tf.int32,\n",
    "                                            initializer=tf.zeros_initializer())\n",
    "\n",
    "            # construct default one hot for all indices to gather from\n",
    "            # as tf.one_hot does not allow specifying depth at runtime\n",
    "            default_indices = tf.get_variable('default_indices',\n",
    "                                            initializer=tf.constant([0,1,2,3,4,\n",
    "                                            5,6,7,8,9,10,11]), trainable=False,\n",
    "                                             dtype=tf.int32)\n",
    "            value_on = tf.get_variable('value_on', dtype=tf.int32,\n",
    "                                            initializer=tf.constant(1),\n",
    "                                            trainable=False)\n",
    "            value_off = tf.get_variable('value_off', dtype=tf.int32,\n",
    "                                            initializer=tf.constant(0),\n",
    "                                            trainable=False)\n",
    "            self.one_hot_default = tf.one_hot(default_indices, 12,\n",
    "                                            on_value=value_on,\n",
    "                                            off_value=value_off)\n",
    "\n",
    "            # the logits given by the trained model\n",
    "            self.eval_logits = tf.placeholder(dtype=tf.float32, shape=[None, 12])\n",
    "\n",
    "            # the correct one hot probability of output\n",
    "            self.eval_output = tf.placeholder(dtype=tf.int32, shape=[None, 12])\n",
    "\n",
    "            # get maximum value of logit and their index for each input\n",
    "            max_values, max_indices = tf.nn.top_k(self.eval_logits, k=1)\n",
    "            self.max_indices = max_indices\n",
    "\n",
    "            # convert indices for maximum logit to one hot encoding\n",
    "            # this operation gathers from one hot default\n",
    "            one_hot = tf.gather_nd(self.one_hot_default, indices=max_indices,\n",
    "                                    name='one_hot')\n",
    "            # self.one_hot = one_hot\n",
    "\n",
    "            # elementwise comparison of the one hot vectors\n",
    "            one_hot_eq = tf.equal(one_hot, self.eval_output)\n",
    "            # self.one_hot_eq = one_hot_eq\n",
    "\n",
    "            # convert to a single truth value for which input\n",
    "            eq_row = tf.reduce_all(one_hot_eq, axis=1)\n",
    "            self.eq_row = eq_row\n",
    "\n",
    "            # convert truth values to scalar and sum to get no of correct preds\n",
    "            correct = tf.reduce_sum(tf.where(eq_row, tf.fill(tf.shape(eq_row), 1),\n",
    "                                                    tf.fill(tf.shape(eq_row), 0)))\n",
    "            # self.correct = correct\n",
    "\n",
    "            # update total accuracy by adding accuracy of current batch\n",
    "            self.acc_upd = self.acc.assign_add(correct)\n",
    "\n",
    "            # get one hot vectors corresponding to correct predictions by model\n",
    "            indices_correct = tf.where(eq_row)\n",
    "            # self.indices_correct = indices_correct\n",
    "            # self.one_hot_correct = one_hot_correct\n",
    "\n",
    "            one_hot_correct = tf.gather(self.eval_output, indices_correct)\n",
    "            # add the gathered vectors to get no of correct predictions per\n",
    "            # category (flair) in current batch\n",
    "            correct_cat = tf.reduce_sum(one_hot_correct, axis=0)\n",
    "            # self.correct_cat = correct_cat\n",
    "\n",
    "            # update accuracy by category by adding accuracy of current batch\n",
    "            self.cat_acc_upd = self.cat_acc.assign_add(correct_cat)\n",
    "\n",
    "            # operations to reset both accuracies\n",
    "            self.acc_reset = tf.assign(self.acc, tf.constant(0))\n",
    "            self.cat_acc_reset = tf.assign(self.cat_acc, tf.zeros(shape=(1,12),\n",
    "                                            dtype=tf.int32))\n",
    "\n",
    "    def get_batch(self, data, batch_no, batch_size):\n",
    "        \"\"\"\n",
    "        Generates to batch by slicing the data\n",
    "\n",
    "        data: data to produce batch from\n",
    "        batch_no: index of batch to produce\n",
    "        batch_size: size of batch to produce\n",
    "        \"\"\"\n",
    "        return data[batch_no * batch_size : (batch_no + 1) * batch_size]\n",
    "\n",
    "    def eval_once(self, sess, write_preds):\n",
    "        \"\"\"\n",
    "        Runs the operations defined in evaluate() to find accuracies\n",
    "\n",
    "        sess: tensorflow session\n",
    "        write_preds: whether to write prediction outcome to file\n",
    "        \"\"\"\n",
    "\n",
    "        # reset accuracies\n",
    "        sess.run([self.acc_reset, self.cat_acc_reset])\n",
    "\n",
    "        # find number of batches\n",
    "        total_batches = ceil(len(self.valid_input) / self.batch_size_valid)\n",
    "\n",
    "        # save start time\n",
    "        start_time_eval = time.time()\n",
    "\n",
    "        # hold prediction outcome for complete validation set\n",
    "        bool_result = []\n",
    "\n",
    "        # compute accuracy over batches\n",
    "        for batch_no in range(total_batches):\n",
    "            # get input and output batch\n",
    "            input_batch = self.get_batch(self.valid_input, batch_no,\n",
    "                                        self.batch_size_valid)\n",
    "            output_batch = np.asarray(self.get_batch(self.valid_output,\n",
    "                                        batch_no, self.batch_size_valid))\n",
    "\n",
    "            # get embeddings for input batch\n",
    "            input_batch_embeds = get_vectors(input_batch, self.en)\n",
    "\n",
    "            # get logits predicted by model\n",
    "            cur_logits = sess.run(self.logits,\n",
    "                                feed_dict={self.input:input_batch_embeds,\n",
    "                                self.dense1_rate:0, self.dense2_rate:0,\n",
    "                                 self.last_output_rate:0})\n",
    "\n",
    "            # get boolean prediction result, and updated accuracies\n",
    "            pred_result, total_acc, cat_acc =  sess.run([self.eq_row, self.acc_upd,\n",
    "                                                        self.cat_acc_upd],\n",
    "                                                        feed_dict={self.eval_logits:cur_logits,\n",
    "                                                        self.eval_output:output_batch})\n",
    "\n",
    "            # save boolean prediction results if want to save pred outcomess\n",
    "            if write_preds == True:\n",
    "                for pred in pred_result:\n",
    "                    bool_result.append(pred)\n",
    "\n",
    "        # write out prediction outcomes to file\n",
    "        if write_preds == True:\n",
    "            with open('results_valid.txt', 'w') as f:\n",
    "                for pred in bool_result:\n",
    "                    f.write(str(pred))\n",
    "                    f.write('\\n')\n",
    "\n",
    "        # get accuracy\n",
    "        validation_acc = total_acc / self.size_valid\n",
    "\n",
    "        # print time to evaluate\n",
    "        print('#debug time to eval: ', time.time()-start_time_eval)\n",
    "\n",
    "        # print total, by flair and percentage accuracies\n",
    "        print('#debug got total correct {} out of {}: '.format(total_acc,\n",
    "                                                            self.size_valid))\n",
    "        print('#debug correct by category: ', cat_acc)\n",
    "        print(\"#debug % accuracy: {}\".format(total_acc / self.size_valid))\n",
    "\n",
    "        # terminate training if achieve desired\n",
    "        # validation accuracy 3 times\n",
    "        if validation_acc > 0.93:\n",
    "            self.above_acc += 1\n",
    "        if self.above_acc == 3:\n",
    "            exit()\n",
    "\n",
    "        # if validation acc exceeds max current\n",
    "        # training going okay and reset overfit counter\n",
    "        if validation_acc > self.max_acc:\n",
    "            self.max_acc = validation_acc\n",
    "            self.overfit = 0\n",
    "\n",
    "        # else increase overfit counter\n",
    "        else:\n",
    "            self.overfit += 1\n",
    "\n",
    "        # terminate training if validation acc does\n",
    "        # not exceed last max for 8 epochs\n",
    "        if self.overfit == 15:\n",
    "            print('Overfit!')\n",
    "            exit()\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Defines summary operations for tensorboard\n",
    "        Summaries for loss per update,\n",
    "        total and by flair accuracy per epoch\n",
    "        average loss per epoch\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "                # summary for loss per weight update\n",
    "                self.loss_summary = tf.summary.scalar('loss', self.loss)\n",
    "\n",
    "                # summaries for total accuracy and accuracy by category per epoch\n",
    "                total_acc_summary = tf.summary.scalar('total_acc', self.acc)\n",
    "                cat0_summary = tf.summary.scalar('cat0_acc', self.cat_acc[0][0])\n",
    "                cat1_summary = tf.summary.scalar('cat1_acc', self.cat_acc[0][1])\n",
    "                cat2_summary = tf.summary.scalar('cat2_acc', self.cat_acc[0][2])\n",
    "                cat3_summary = tf.summary.scalar('cat3_acc', self.cat_acc[0][3])\n",
    "                cat4_summary = tf.summary.scalar('cat4_acc', self.cat_acc[0][4])\n",
    "                cat5_summary = tf.summary.scalar('cat5_acc', self.cat_acc[0][5])\n",
    "                cat6_summary = tf.summary.scalar('cat6_acc', self.cat_acc[0][6])\n",
    "                cat7_summary = tf.summary.scalar('cat7_acc', self.cat_acc[0][7])\n",
    "                cat8_summary = tf.summary.scalar('cat8_acc', self.cat_acc[0][8])\n",
    "                cat9_summary = tf.summary.scalar('cat9_acc', self.cat_acc[0][9])\n",
    "                cat10_summary = tf.summary.scalar('cat10_acc', self.cat_acc[0][10])\n",
    "                cat11_summary = tf.summary.scalar('cat11_acc', self.cat_acc[0][11])\n",
    "                self.summary_acc = tf.summary.merge([cat0_summary, cat1_summary,\n",
    "                                                    cat2_summary, cat3_summary,\n",
    "                                                    cat4_summary, cat5_summary,\n",
    "                                                    cat6_summary, cat7_summary,\n",
    "                                                    cat8_summary, cat9_summary,\n",
    "                                                    cat10_summary, cat11_summary,\n",
    "                                                    total_acc_summary])\n",
    "\n",
    "                # summary for average loss per epoch\n",
    "                self.loss_avg_summary = tf.summary.scalar('loss_avg', self.loss_avg)\n",
    "\n",
    "    def loss_avg(self):\n",
    "        \"\"\" Defines operations to compute average loss per epoch \"\"\"\n",
    "        # placeholders for total loss and number of batches\n",
    "        self.total_loss = tf.placeholder(dtype=tf.float32, shape=())\n",
    "        self.total_batches = tf.placeholder(dtype=tf.float32, shape=())\n",
    "\n",
    "        # compute average loss\n",
    "        self.loss_avg = self.total_loss / self.total_batches\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Builds the model \"\"\"\n",
    "        self.get_data()\n",
    "        self.inference()\n",
    "        self.loss()\n",
    "        self.optimize()\n",
    "        self.evaluate()\n",
    "        self.loss_avg()\n",
    "        self.summary()\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Trains the model\n",
    "\n",
    "        num_epochs: number of epochs to train\n",
    "        \"\"\"\n",
    "\n",
    "        # make directories to save weights and summaries\n",
    "        safe_mkdir('checkpoints')\n",
    "        safe_mkdir('checkpoints/flair')\n",
    "        writer = tf.summary.FileWriter('./graphs/flair', tf.get_default_graph())\n",
    "\n",
    "        # start a tensorflow session\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # initialize global variables and get weight saver\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            # load saved weights if present\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname(\n",
    "                                                'checkpoints/flair/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print('#debug: loaded saved model!')\n",
    "\n",
    "            # get step count for tensorboard summaries\n",
    "            step = self.gstep.eval()\n",
    "            step_acc = self.gstep_acc.eval()\n",
    "\n",
    "            # Convert dataset to input output tuples to facilitate shuffling\n",
    "            data = []\n",
    "            for index in range(len(self.train_input)):\n",
    "                data.append((self.train_input[index], self.train_output[index]))\n",
    "\n",
    "\n",
    "            # evaluate before training\n",
    "            self.eval_once(sess, False)\n",
    "\n",
    "            # uncomment to test\n",
    "            #exit()\n",
    "\n",
    "            # iterate over epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                # change mode to training\n",
    "                #self.change_mode(saver, sess, 'train')\n",
    "\n",
    "                # get number of training batches and print it\n",
    "                total_batches = ceil(len(self.train_input) / self.batch_size_train)\n",
    "                print('#debug epoch: {} no of batches: {}'.format(epoch, total_batches))\n",
    "\n",
    "                # hold total loss for epoch\n",
    "                total_loss = 0\n",
    "\n",
    "                # shuffle the dataset\n",
    "                random.shuffle(data)\n",
    "\n",
    "                # get separate input and output arrays from tuples\n",
    "                data_input = []\n",
    "                data_output = []\n",
    "                for index in range(len(data)):\n",
    "                    data_input.append(data[index][0])\n",
    "                    data_output.append(data[index][1])\n",
    "\n",
    "                # save start time\n",
    "                start_time = time.time()\n",
    "\n",
    "                # iterate over batches\n",
    "                for batch_no in range(total_batches):\n",
    "\n",
    "                    # get input and output batch\n",
    "                    input_batch = self.get_batch(data_input, batch_no,\n",
    "                                                self.batch_size_train)\n",
    "                    output_batch = np.asarray(self.get_batch(data_output, batch_no,\n",
    "                                            self.batch_size_train), dtype=np.int32)\n",
    "\n",
    "                    # get embeddings for input batch\n",
    "                    input_batch_embeds = get_vectors(input_batch, self.en)\n",
    "\n",
    "                    # run optimizer, get loss and loss summary\n",
    "                    _, loss, loss_summary = sess.run([self.opt, self.loss,\n",
    "                                                    self.loss_summary],\n",
    "                                                    feed_dict={self.input:input_batch_embeds,\n",
    "                                                    self.output:output_batch,\n",
    "                                                    self.dense1_rate:0,\n",
    "                                                    self.dense2_rate:0,\n",
    "                                                    self.last_output_rate:0})\n",
    "\n",
    "                    # add loss summary per update\n",
    "                    writer.add_summary(loss_summary, global_step=step)\n",
    "\n",
    "                    # update total loss and step for total loss\n",
    "                    total_loss += loss\n",
    "                    step += 1\n",
    "\n",
    "                    # print loss at regular intervals\n",
    "                    if batch_no % self.skip_step == 0:\n",
    "                        print('#debug loss: ', loss)\n",
    "\n",
    "                # print time to train epoch and average loss\n",
    "                print('time to train one epoch {} : {}'.format(epoch, time.time()\n",
    "                                                                - start_time))\n",
    "                print(\"average loss at epoch {} : {}\".format(epoch, total_loss\n",
    "                                                                / total_batches))\n",
    "\n",
    "                # save weights at every epoch\n",
    "                saver.save(sess, 'checkpoints/flair/checkpoint', step)\n",
    "\n",
    "                # get validation accuracy at every epoch\n",
    "                self.eval_once(sess, write_preds=False)\n",
    "\n",
    "                # get summaries for accuracies and average loss per epoch\n",
    "                summary_acc = sess.run(self.summary_acc)\n",
    "                summary_loss_avg = sess.run(self.loss_avg_summary,\n",
    "                                            feed_dict={self.total_loss:total_loss,\n",
    "                                             self.total_batches:total_batches})\n",
    "\n",
    "                # add average loss and accuracies summaries\n",
    "                writer.add_summary(summary_loss_avg, global_step=step_acc)\n",
    "                writer.add_summary(summary_acc, global_step=step_acc)\n",
    "\n",
    "                # update step for summaries per epoch\n",
    "                step_acc += 1\n",
    "\n",
    "            # uncomment to write validation preds at end of training\n",
    "            #self.eval_once(sess, write_preds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trying to share variable evaluate/default_indices, but specified dtype float32 and found dtype int32_ref.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-04f5d5e91741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# get the model object and build the computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlairPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#debug: model built!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-70ebfe8e9526>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-70ebfe8e9526>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             default_indices = tf.get_variable('default_indices',\n\u001b[1;32m    138\u001b[0m                                             initializer=tf.constant([0,1,2,3,4,\n\u001b[0;32m--> 139\u001b[0;31m                                             5,6,7,8,9,10,11]), trainable=False)\n\u001b[0m\u001b[1;32m    140\u001b[0m             value_on = tf.get_variable('value_on', dtype=tf.int32,\n\u001b[1;32m    141\u001b[0m                                             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1477\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1218\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    545\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    497\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    857\u001b[0m         raise ValueError(\"Trying to share variable %s, but specified dtype %s\"\n\u001b[1;32m    858\u001b[0m                          \" and found dtype %s.\" % (name, dtype_str,\n\u001b[0;32m--> 859\u001b[0;31m                                                    found_type_str))\n\u001b[0m\u001b[1;32m    860\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfound_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to share variable evaluate/default_indices, but specified dtype float32 and found dtype int32_ref."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set random seeds\n",
    "    #seed = 10\n",
    "    #seed = 83\n",
    "    #seed = 152\n",
    "    #np.random.seed(seed)\n",
    "    #tf.set_random_seed(seed)\n",
    "    #random.seed(seed)\n",
    "\n",
    "    # get the model object and build the computation graph\n",
    "    model = FlairPredict()\n",
    "    model.build()\n",
    "    print('#debug: model built!')\n",
    "\n",
    "    # train the model\n",
    "    model.train(150)\n",
    "    print('#debug: model trained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
